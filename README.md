# ğŸ¯ Modelo Behavior Claro - PrediÃ§Ã£o de Risco de CrÃ©dito

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![AWS](https://img.shields.io/badge/AWS-Cloud-orange.svg)](https://aws.amazon.com/)
[![Status](https://img.shields.io/badge/Status-Em%20Desenvolvimento-yellow.svg)]()
[![Hackathon](https://img.shields.io/badge/Super%20Hackathon-2025-orange.svg)]()
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

> Modelo de prediÃ§Ã£o de risco de inadimplÃªncia para clientes prÃ©-pagos baseado em visÃ£o Ãºnica por CPF utilizando arquitetura Medallion na AWS

---

## ğŸ“‘ Ãndice

- [Sobre o Projeto](#-sobre-o-projeto)
- [Problema de NegÃ³cio](#-problema-de-negÃ³cio)
- [Arquitetura da SoluÃ§Ã£o](#-arquitetura-da-soluÃ§Ã£o)
- [Stack TecnolÃ³gico](#-stack-tecnolÃ³gico)
- [Estrutura do Projeto](#-estrutura-do-projeto)
- [Como Reproduzir](#-como-reproduzir)
- [Pipeline de Dados](#-pipeline-de-dados)
- [Modelagem](#-modelagem)
- [ValidaÃ§Ã£o e MÃ©tricas](#-validaÃ§Ã£o-e-mÃ©tricas)
- [SeguranÃ§a e GovernanÃ§a](#-seguranÃ§a-e-governanÃ§a)
- [Resultados](#-resultados)
- [Time](#-time)
- [DocumentaÃ§Ã£o Completa](#-documentaÃ§Ã£o-completa)
- [LicenÃ§a](#-licenÃ§a)

---

## ğŸ¯ Sobre o Projeto

Este projeto foi desenvolvido durante o **Super Hackathon 2025** pela **PoD Academy** com o objetivo de criar um modelo preditivo de risco de crÃ©dito para identificar clientes da base prÃ©-pago da Claro com maior probabilidade de se tornarem inadimplentes.

### Objetivos Principais

- âœ… Construir visÃ£o Ãºnica de cliente por CPF consolidando mÃºltiplas bases
- âœ… Identificar padrÃµes comportamentais de inadimplÃªncia
- âœ… Desenvolver modelo behavior reprodutÃ­vel e escalÃ¡vel
- âœ… Gerar insights acionÃ¡veis para estratÃ©gias de mitigaÃ§Ã£o de risco
- âœ… Implementar arquitetura moderna e governada na AWS

### Contexto de NegÃ³cio

A inadimplÃªncia em serviÃ§os de telecomunicaÃ§Ãµes representa um desafio crÃ­tico para a operaÃ§Ã£o. A capacidade de identificar preventivamente clientes com maior risco permite:

- **ReduÃ§Ã£o de perdas** financeiras por inadimplÃªncia
- **OtimizaÃ§Ã£o de polÃ­ticas** de crÃ©dito e cobranÃ§a hiper-segmentadas
- **PersonalizaÃ§Ã£o de ofertas** baseada em perfil de risco
- **Melhoria da experiÃªncia** do cliente atravÃ©s de abordagens proativas
- **Captura de oportunidades** de negÃ³cio (migraÃ§Ã£o prÃ© â†’ pÃ³s-pago)

---

## ğŸ’¼ Problema de NegÃ³cio

### Desafio

Identificar, entre os clientes da base prÃ©-pago, quais tÃªm maior probabilidade de se tornarem inadimplentes, permitindo aÃ§Ãµes preventivas e estratÃ©gias de mitigaÃ§Ã£o de risco.

> **"Risco de CrÃ©dito Ã© a chance de famÃ­lias e empresas NÃƒO cumprirem com suas obrigaÃ§Ãµes, resultando em perdas."**

### Contexto EstratÃ©gico da Claro

**DimensÃ£o do Desafio**:
- ğŸ¯ **50 milhÃµes de assinantes** na base
- ğŸ“Š **Taxa de aprovaÃ§Ã£o atual**: 73-74%
- ğŸ² **Grupo controle**: ~2% da base (seleÃ§Ã£o por 6Âº e 7Âº dÃ­gitos do CPF)
- ğŸ”„ **AnÃ¡lise contÃ­nua** influenciada por fatores de mercado, regulaÃ§Ã£o e economia

**Objetivo EstratÃ©gico**:
- **CenÃ¡rio 1**: Aumentar taxa de aprovaÃ§Ã£o **mantendo** inadimplÃªncia estÃ¡vel
- **CenÃ¡rio 2**: Reduzir inadimplÃªncia **mantendo** taxa de aprovaÃ§Ã£o estÃ¡vel
- **Ideal**: Ganho incremental em ambas as dimensÃµes atravÃ©s de polÃ­ticas hiper-segmentadas

### Abordagem

ConsolidaÃ§Ã£o de dados cadastrais, comportamentais, de score de crÃ©dito e histÃ³rico de pagamentos em uma **visÃ£o Ãºnica por CPF**, seguida de modelagem preditiva incremental para classificaÃ§Ã£o de risco.

**Diferenciais da SoluÃ§Ã£o**:
- âœ… **CrÃ©dito Telco â‰  CrÃ©dito Tradicional**: Uso de dados comportamentais de recarga
- âœ… **Modelagem Incremental**: AvaliaÃ§Ã£o de ganho por fonte de dados
- âœ… **Foco em Oportunidades**: Identificar clientes "escondidos" com bom comportamento
- âœ… **GestÃ£o de Risco como Alavanca**: Transformar controle em captura de negÃ³cios

### Case de Sucesso Real

**SituaÃ§Ã£o**: Cliente prÃ©-pago com histÃ³rico consistente de recarga, mas sem histÃ³rico bancÃ¡rio robusto

**Problema**: Modelo tradicional classificaria como **alto risco** â†’ Cliente negado para pÃ³s-pago

**SoluÃ§Ã£o**: AnÃ¡lise de padrÃ£o de recarga permite reclassificaÃ§Ã£o de Alto â†’ MÃ©dio/Baixo risco

**Resultado**: âœ… Cliente aprovado, âœ… RetenÃ§Ã£o, âœ… Aumento de receita (pÃ³s > prÃ©)

> *"Ã‰ diferente de crÃ©dito pessoal, financiamento... com as informaÃ§Ãµes de recarga eu consigo classificar ou eventualmente reclassificar um cliente que nÃ£o teria chance de ter pÃ³s-pago."*

### MÃ©tricas de Sucesso

**Benchmark Estabelecido (Out-of-Time Fev/Mar)**:
- **KS (Kolmogorov-Smirnov)**: 33,1 â­ (mÃ©trica principal)
- **GINI**: Complementar ao KS
- **Taxa de AprovaÃ§Ã£o**: ~73-74%
- **InadimplÃªncia**: Baseline do grupo controle

**MÃ©tricas de AvaliaÃ§Ã£o**:

| MÃ©trica | DescriÃ§Ã£o | Objetivo |
|---------|-----------|----------|
| **KS Statistic** | Poder de separaÃ§Ã£o entre bons e maus pagadores | â‰¥ 33,1 (benchmark) |
| **GINI** | Ãrea sob curva de Lorenz | MÃ¡ximo possÃ­vel |
| **Curva ROC / AUC** | DiscriminaÃ§Ã£o entre classes | AnÃ¡lise completa da curva |
| **Swap In / Swap Out** | AnÃ¡lise de cenÃ¡rios de troca de polÃ­tica | Ganho incremental |
| **Taxa de AprovaÃ§Ã£o** | % de clientes aprovados | 73-74% (baseline) |
| **Taxa de InadimplÃªncia** | % de inadimplentes | Baseline grupo controle |

**âš ï¸ Importante**: 
> "O KS NÃƒO Ã‰ TUDO NESSA VIDA" - A anÃ¡lise deve considerar toda a curva ROC e matriz de confusÃ£o, com **foco especial na metade inferior da curva de score** (onde estÃ¡ o maior impacto de negÃ³cio).

### EstratÃ©gia de Modelagem Incremental

A Claro utiliza abordagem de **modelagem incremental** para avaliar viabilidade econÃ´mica de cada fonte de dados:

```
1. Baseline (CPF + Safra + Target)           â†’ KS base
2. + Book Atraso/Pagamento                   â†’ Medir ganho Î”KS
3. + Book Recarga                            â†’ Medir ganho Î”KS
4. + Dados Cadastrais                        â†’ Medir ganho Î”KS
5. + Dados TELCO                             â†’ Medir ganho Î”KS
6. + Scores Bureau Externos                  â†’ KS final
```

**Justificativa**: Viabilizar decisÃµes de investimento em aquisiÃ§Ã£o de dados externos baseadas em ROI de cada fonte.

**Exemplo Real de Ganho**:
- KS inicial (Book Atraso/Pagamento): **15**
- + Book Recarga: **25** (ganho de **10 pontos**)
- Objetivo final: **33+** (benchmark)

---

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

![Arquitetura AWS](diagrama_arquitetura.jpg)

### VisÃ£o Geral da Arquitetura

A soluÃ§Ã£o foi implementada seguindo arquitetura **Medallion** (Bronze â†’ Silver â†’ Gold) na AWS, garantindo qualidade, governanÃ§a e escalabilidade dos dados.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          EXTERNAL DATA SOURCES                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Oracle DB  â”‚ PostgreSQL   â”‚   APIs       â”‚   CSV/Parquet Files        â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚              â”‚              â”‚              â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚         AWS LAMBDA (IngestÃ£o)            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    ğŸ¥‰ BRONZE LAYER (S3 - Raw Data)       â”‚
       â”‚    â€¢ Dados brutos em Parquet/JSON        â”‚
       â”‚    â€¢ Particionamento por data            â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚       AWS GLUE CRAWLER                   â”‚
       â”‚       (CatalogaÃ§Ã£o AutomÃ¡tica)           â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚         AWS GLUE DATA CATALOG            â”‚
       â”‚         (Metadados Centralizados)        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                             â”‚
           â–¼                             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AWS LAMBDA     â”‚         â”‚   AMAZON EMR     â”‚
  â”‚  (Light ETL)    â”‚         â”‚   (Heavy ETL)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    ğŸ¥ˆ SILVER LAYER (S3 - Clean Data)     â”‚
       â”‚    â€¢ Data Quality aplicado               â”‚
       â”‚    â€¢ DeduplicaÃ§Ã£o                        â”‚
       â”‚    â€¢ NormalizaÃ§Ã£o                        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                             â”‚
           â–¼                             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AWS LAMBDA     â”‚         â”‚   AMAZON EMR     â”‚
  â”‚  (Aggregation)  â”‚         â”‚ (Feature Eng.)   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚    ğŸ¥‡ GOLD LAYER (S3 - Curated Data)     â”‚
       â”‚    â€¢ VisÃ£o Ãšnica por CPF                 â”‚
       â”‚    â€¢ Features Engineered                 â”‚
       â”‚    â€¢ Pronto para Consumo                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                             â”‚
           â–¼                             â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AMAZON         â”‚         â”‚   AMAZON         â”‚
  â”‚  SAGEMAKER      â”‚         â”‚   REDSHIFT       â”‚
  â”‚  (ML Training)  â”‚         â”‚   (DW Analytics) â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                           â”‚
           â”‚                           â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                         â”‚
           â–¼                         â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  AMAZON ATHENA  â”‚     â”‚    POWER BI      â”‚
  â”‚  (SQL Queries)  â”‚     â”‚  (Dashboards)    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes da Arquitetura

#### **1. Camada de IngestÃ£o**

| Componente | FunÃ§Ã£o | Tecnologia |
|------------|--------|------------|
| **External Sources** | Fontes de dados | Oracle, PostgreSQL, APIs, Files |
| **AWS Lambda** | IngestÃ£o serverless | Python |
| **S3 Bronze** | Storage de dados brutos | Parquet, JSON |

#### **2. Camada de CatalogaÃ§Ã£o**

| Componente | FunÃ§Ã£o | Tecnologia |
|------------|--------|------------|
| **Glue Crawler** | Descoberta automÃ¡tica de esquemas | AWS Glue |
| **Data Catalog** | Metadados centralizados | AWS Glue |

#### **3. Camada de TransformaÃ§Ã£o**

| Componente | FunÃ§Ã£o | Tecnologia |
|------------|--------|------------|
| **AWS Lambda** | ETL leve (< 15 min) | Python |
| **Amazon EMR** | ETL pesado distribuÃ­do | PySpark |
| **S3 Silver** | Dados limpos e validados | Parquet |

#### **4. Camada de AgregaÃ§Ã£o**

| Componente | FunÃ§Ã£o | Tecnologia |
|------------|--------|------------|
| **AWS Lambda** | AgregaÃ§Ãµes simples | Python |
| **Amazon EMR** | Feature Engineering complexo | PySpark |
| **S3 Gold** | Dados prontos para ML | Parquet |

#### **5. Camada de Consumo**

| Componente | FunÃ§Ã£o | Tecnologia |
|------------|--------|------------|
| **Amazon SageMaker** | Treinamento de modelos | Scikit-learn, XGBoost |
| **Amazon Redshift** | Data Warehouse | SQL |
| **Amazon Athena** | Queries ad-hoc | SQL |
| **Power BI** | VisualizaÃ§Ã£o e dashboards | DAX, Power Query |

#### **6. SeguranÃ§a e GovernanÃ§a**

| Componente | FunÃ§Ã£o |
|------------|--------|
| **AWS IAM** | Controle de acesso (3 perfis: DecisÃ£o, AnalÃ­tico, Operacional) |
| **AWS KMS** | Criptografia de dados |
| **AWS CloudWatch** | Monitoramento e logs |

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### **Armazenamento e Data Lake**
- **Amazon S3** - Data Lake (Bronze, Silver, Gold)
- **Amazon Redshift** - Data Warehouse
- **Formatos**: Parquet (otimizado), JSON, CSV

### **Processamento de Dados**
- **AWS Lambda** - Processamento serverless (Python 3.8+)
- **Amazon EMR** - Processamento distribuÃ­do (PySpark)
- **Polars** - Processamento de dados de alta performance
- **Pandas** - AnÃ¡lise e manipulaÃ§Ã£o de dados

### **CatalogaÃ§Ã£o e GovernanÃ§a**
- **AWS Glue Crawler** - Descoberta automÃ¡tica de esquemas
- **AWS Glue Data Catalog** - Metadados centralizados
- **Amazon Athena** - Queries SQL serverless

### **Machine Learning**
- **Amazon SageMaker** - Treinamento e deployment
- **Scikit-learn** - Modelagem e avaliaÃ§Ã£o
- **Jupyter Notebooks** - ExperimentaÃ§Ã£o e anÃ¡lise

### **VisualizaÃ§Ã£o e BI**
- **Power BI** - Dashboards interativos corporativos
- **AWS QuickSight** - VisualizaÃ§Ãµes em nuvem
- **Excel** - AnÃ¡lises ad-hoc
- **PowerPoint** - ApresentaÃ§Ãµes

### **SeguranÃ§a e Monitoramento**
- **AWS IAM** - GestÃ£o de identidades e acessos
- **AWS KMS** - Key Management Service (criptografia)
- **AWS CloudWatch** - Monitoramento, logs e alertas

### **Linguagens e Ferramentas**
- **Python 3.8+** - Linguagem principal
- **PySpark** - Processamento distribuÃ­do
- **SQL** - Queries e transformaÃ§Ãµes

---

## ğŸ“‚ Estrutura do Projeto

```
modelo-behavior-claro/
â”‚
â”œâ”€â”€ README.md                          # Este arquivo
â”œâ”€â”€ requirements.txt                   # DependÃªncias Python
â”œâ”€â”€ environment.yml                    # Ambiente Conda (opcional)
â”‚
â”œâ”€â”€ docs/                              # DocumentaÃ§Ã£o detalhada
â”‚   â”œâ”€â”€ 01_business_context.md        # Contexto de negÃ³cio
â”‚   â”œâ”€â”€ 02_data_understanding.md      # Entendimento dos dados
â”‚   â”œâ”€â”€ 03_eda_insights.md            # Insights da anÃ¡lise exploratÃ³ria
â”‚   â”œâ”€â”€ 04_data_preparation.md        # PreparaÃ§Ã£o e limpeza
â”‚   â”œâ”€â”€ 05_feature_engineering.md     # Engenharia de features
â”‚   â”œâ”€â”€ 06_modeling.md                # Processo de modelagem
â”‚   â”œâ”€â”€ 07_evaluation.md              # AvaliaÃ§Ã£o e mÃ©tricas
â”‚   â”œâ”€â”€ 08_architecture.md            # Arquitetura detalhada
â”‚   â”œâ”€â”€ 09_security_governance.md     # SeguranÃ§a e governanÃ§a
â”‚   â””â”€â”€ data_dictionary.xlsx          # DicionÃ¡rio de dados completo
â”‚
â”œâ”€â”€ notebooks/                         # Jupyter Notebooks
â”‚   â”œâ”€â”€ 01_eda_cadastrais.ipynb       # EDA - Dados cadastrais
â”‚   â”œâ”€â”€ 02_eda_score_bureau.ipynb     # EDA - Score de crÃ©dito
â”‚   â”œâ”€â”€ 03_eda_telco.ipynb            # EDA - Dados telco
â”‚   â”œâ”€â”€ 04_eda_recarga_pagamento.ipynb # EDA - Recarga e pagamento
â”‚   â”œâ”€â”€ 05_data_integration.ipynb     # IntegraÃ§Ã£o de bases
â”‚   â”œâ”€â”€ 06_feature_engineering.ipynb  # CriaÃ§Ã£o de features
â”‚   â”œâ”€â”€ 07_incremental_modeling.ipynb # Modelagem incremental
â”‚   â””â”€â”€ 08_model_evaluation.ipynb     # AvaliaÃ§Ã£o do modelo
â”‚
â”œâ”€â”€ src/                               # CÃ³digo fonte modularizado
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ lambda/                        # FunÃ§Ãµes Lambda
â”‚   â”‚   â”œâ”€â”€ ingestion/                # IngestÃ£o de dados
â”‚   â”‚   â”‚   â”œâ”€â”€ oracle_to_s3.py
â”‚   â”‚   â”‚   â”œâ”€â”€ postgres_to_s3.py
â”‚   â”‚   â”‚   â””â”€â”€ api_to_s3.py
â”‚   â”‚   â”œâ”€â”€ transformation/           # TransformaÃ§Ãµes leves
â”‚   â”‚   â”‚   â””â”€â”€ silver_processor.py
â”‚   â”‚   â””â”€â”€ aggregation/              # AgregaÃ§Ãµes
â”‚   â”‚       â””â”€â”€ gold_processor.py
â”‚   â”‚
â”‚   â”œâ”€â”€ glue/                          # Glue Jobs
â”‚   â”‚   â”œâ”€â”€ crawlers/                 # ConfiguraÃ§Ã£o de crawlers
â”‚   â”‚   â””â”€â”€ jobs/                     # ETL jobs
â”‚   â”‚       â”œâ”€â”€ bronze_to_silver.py
â”‚   â”‚       â””â”€â”€ silver_to_gold.py
â”‚   â”‚
â”‚   â”œâ”€â”€ emr/                           # Scripts EMR/PySpark
â”‚   â”‚   â”œâ”€â”€ data_cleaning.py
â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”‚   â””â”€â”€ data_integration.py
â”‚   â”‚
â”‚   â”œâ”€â”€ sagemaker/                     # Scripts SageMaker
â”‚   â”‚   â”œâ”€â”€ train.py                  # Pipeline de treinamento
â”‚   â”‚   â”œâ”€â”€ evaluate.py               # AvaliaÃ§Ã£o de modelos
â”‚   â”‚   â””â”€â”€ deploy.py                 # Deploy do modelo
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ builder.py                # ConstruÃ§Ã£o de features
â”‚   â”‚   â””â”€â”€ selector.py               # SeleÃ§Ã£o de features
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ baseline.py               # Modelo baseline
â”‚   â”‚   â””â”€â”€ incremental.py            # Modelagem incremental
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config.py                 # ConfiguraÃ§Ãµes
â”‚       â”œâ”€â”€ s3_utils.py               # UtilitÃ¡rios S3
â”‚       â””â”€â”€ logger.py                 # Logging
â”‚
â”œâ”€â”€ infrastructure/                    # Infraestrutura como cÃ³digo
â”‚   â”œâ”€â”€ cloudformation/               # Templates CloudFormation
â”‚   â”‚   â”œâ”€â”€ s3_buckets.yaml
â”‚   â”‚   â”œâ”€â”€ lambda_functions.yaml
â”‚   â”‚   â”œâ”€â”€ glue_resources.yaml
â”‚   â”‚   â”œâ”€â”€ emr_cluster.yaml
â”‚   â”‚   â””â”€â”€ sagemaker_resources.yaml
â”‚   â””â”€â”€ terraform/                    # Alternativa Terraform (opcional)
â”‚
â”œâ”€â”€ sql/                               # Queries SQL
â”‚   â”œâ”€â”€ athena/                       # Queries Athena
â”‚   â”‚   â”œâ”€â”€ data_quality_checks.sql
â”‚   â”‚   â””â”€â”€ feature_queries.sql
â”‚   â””â”€â”€ redshift/                     # Queries Redshift
â”‚       â””â”€â”€ aggregations.sql
â”‚
â”œâ”€â”€ data/                              # Dados locais (gitignored)
â”‚   â”œâ”€â”€ bronze/                       # Dados brutos (raw)
â”‚   â”œâ”€â”€ silver/                       # Dados limpos
â”‚   â””â”€â”€ gold/                         # Dados agregados/features
â”‚
â”œâ”€â”€ models/                            # Modelos treinados e artifacts
â”‚   â”œâ”€â”€ baseline/
â”‚   â”œâ”€â”€ incremental/
â”‚   â””â”€â”€ final/
â”‚
â”œâ”€â”€ reports/                           # RelatÃ³rios e visualizaÃ§Ãµes
â”‚   â”œâ”€â”€ figures/                      # GrÃ¡ficos e diagramas
â”‚   â”‚   â””â”€â”€ architecture_diagram.png
â”‚   â”œâ”€â”€ dashboards/                   # Dashboards Power BI
â”‚   â””â”€â”€ presentations/                # ApresentaÃ§Ãµes
â”‚
â””â”€â”€ tests/                             # Testes unitÃ¡rios
    â”œâ”€â”€ test_lambda_functions.py
    â”œâ”€â”€ test_data_cleaning.py
    â”œâ”€â”€ test_feature_engineering.py
    â””â”€â”€ test_model_training.py
```

---

## ğŸš€ Como Reproduzir

### PrÃ©-requisitos

- Python 3.8 ou superior
- Conta AWS com acesso a: S3, Lambda, Glue, EMR, SageMaker, Redshift, Athena
- AWS CLI configurado
- PermissÃµes IAM adequadas

### InstalaÃ§Ã£o

#### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/modelo-behavior-claro.git
cd modelo-behavior-claro
```

#### 2. Crie ambiente virtual

```bash
# Usando venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows

# Ou usando conda
conda env create -f environment.yml
conda activate behavior-claro
```

#### 3. Instale as dependÃªncias

```bash
pip install -r requirements.txt
```

#### 4. Configure credenciais AWS

```bash
# Configure suas credenciais AWS
aws configure

# Ou exporte as variÃ¡veis de ambiente
export AWS_ACCESS_KEY_ID="sua_access_key"
export AWS_SECRET_ACCESS_KEY="sua_secret_key"
export AWS_DEFAULT_REGION="us-east-1"
```

#### 5. Configure variÃ¡veis de ambiente

```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Edite com suas configuraÃ§Ãµes
vim .env
```

Exemplo de `.env`:
```bash
# S3 Buckets (Medallion Architecture)
S3_BUCKET_BRONZE=s3://claro-hackathon-bronze/
S3_BUCKET_SILVER=s3://claro-hackathon-silver/
S3_BUCKET_GOLD=s3://claro-hackathon-gold/
S3_BUCKET_MODELS=s3://claro-hackathon-models/

# AWS Resources
GLUE_DATABASE=claro_credit_risk
GLUE_CRAWLER_NAME=claro-bronze-crawler
EMR_CLUSTER_ID=j-XXXXXXXXXXXXX
REDSHIFT_CLUSTER=claro-dw
REDSHIFT_DATABASE=analytics

# IAM Roles
LAMBDA_ROLE_ARN=arn:aws:iam::XXXX:role/LambdaExecutionRole
GLUE_ROLE_ARN=arn:aws:iam::XXXX:role/GlueServiceRole
SAGEMAKER_ROLE_ARN=arn:aws:iam::XXXX:role/SageMakerRole
EMR_ROLE_ARN=arn:aws:iam::XXXX:role/EMRServiceRole

# SeguranÃ§a
KMS_KEY_ID=arn:aws:kms:us-east-1:XXXX:key/YYYY
```

### Provisionamento da Infraestrutura

#### OpÃ§Ã£o 1: CloudFormation (Recomendado)

```bash
# Deploy de todos os recursos
cd infrastructure/cloudformation

# 1. Criar buckets S3
aws cloudformation deploy \
  --template-file s3_buckets.yaml \
  --stack-name claro-s3-stack

# 2. Deploy funÃ§Ãµes Lambda
aws cloudformation deploy \
  --template-file lambda_functions.yaml \
  --stack-name claro-lambda-stack \
  --capabilities CAPABILITY_IAM

# 3. Configurar Glue resources
aws cloudformation deploy \
  --template-file glue_resources.yaml \
  --stack-name claro-glue-stack \
  --capabilities CAPABILITY_IAM

# 4. Provisionar cluster EMR
aws cloudformation deploy \
  --template-file emr_cluster.yaml \
  --stack-name claro-emr-stack \
  --capabilities CAPABILITY_IAM

# 5. Setup SageMaker
aws cloudformation deploy \
  --template-file sagemaker_resources.yaml \
  --stack-name claro-sagemaker-stack \
  --capabilities CAPABILITY_IAM
```

#### OpÃ§Ã£o 2: Manual via Console AWS

1. **S3**: Criar 4 buckets (bronze, silver, gold, models)
2. **IAM**: Criar roles para Lambda, Glue, EMR, SageMaker
3. **Lambda**: Deploy das funÃ§Ãµes de ingestÃ£o
4. **Glue**: Criar database e crawlers
5. **EMR**: Provisionar cluster
6. **SageMaker**: Criar notebook instance
7. **Redshift**: Provisionar cluster (opcional)

### ExecuÃ§Ã£o do Pipeline

#### OpÃ§Ã£o 1: Pipeline Completo via Lambda + Glue

```bash
# 1. Trigger ingestÃ£o de dados (Lambda)
aws lambda invoke \
  --function-name claro-ingest-data \
  --payload '{"source": "all"}' \
  response.json

# 2. Executar Glue Crawler
aws glue start-crawler --name claro-bronze-crawler

# 3. Aguardar conclusÃ£o do crawler
aws glue get-crawler --name claro-bronze-crawler

# 4. Executar job Bronze â†’ Silver (Glue)
aws glue start-job-run --job-name bronze-to-silver-job

# 5. Executar job Silver â†’ Gold (EMR)
aws emr add-steps \
  --cluster-id $EMR_CLUSTER_ID \
  --steps Type=Spark,Name="Silver to Gold",ActionOnFailure=CONTINUE,Args=[--deploy-mode,cluster,--master,yarn,s3://claro-hackathon-code/emr/silver_to_gold.py]
```

#### OpÃ§Ã£o 2: Processamento via EMR (Desenvolvimento)

```bash
# Submeter job PySpark ao EMR
aws emr add-steps \
  --cluster-id $EMR_CLUSTER_ID \
  --steps Type=Spark,Name="Feature Engineering",ActionOnFailure=CONTINUE,Args=[s3://claro-hackathon-code/emr/feature_engineering.py]

# Monitorar execuÃ§Ã£o
aws emr describe-step --cluster-id $EMR_CLUSTER_ID --step-id s-XXXXXXXXXXXXX
```

#### OpÃ§Ã£o 3: Notebooks Jupyter (ExploraÃ§Ã£o)

```bash
# Inicie o Jupyter Lab
jupyter lab

# Execute os notebooks na ordem:
# 01_eda_*.ipynb â†’ 05_data_integration.ipynb â†’ 
# 06_feature_engineering.ipynb â†’ 07_incremental_modeling.ipynb â†’ 
# 08_model_evaluation.ipynb
```

#### OpÃ§Ã£o 4: SageMaker Training

```bash
# Submeter job de treinamento
python src/sagemaker/train.py \
  --input-data s3://claro-hackathon-gold/features/ \
  --output-model s3://claro-hackathon-models/baseline/
```

---

## ğŸ”„ Pipeline de Dados

### VisÃ£o Geral do Fluxo

```
External Sources â†’ Bronze (Raw) â†’ Silver (Clean) â†’ Gold (Curated) â†’ ML Models
```

### 1. IngestÃ£o de Dados (Sources â†’ Bronze)

**Fontes de Dados da Claro:**

| Base | DescriÃ§Ã£o | Granularidade | Volume | Storage |
|------|-----------|---------------|--------|---------|
| `base_dados_cadastrais` | InformaÃ§Ãµes cadastrais | CPF | ~10M | Oracle â†’ S3 Bronze |
| `base_score_bureau_movel` | Score de crÃ©dito | CPF + Data | ~15M | PostgreSQL â†’ S3 Bronze |
| `base_score_bureau_movel_full` | Score completo | CPF + Data | ~20M | PostgreSQL â†’ S3 Bronze |
| `base_telco` | Uso de serviÃ§os | CPF + MÃªs | ~50M | API â†’ S3 Bronze |
| `bases_recarga` | HistÃ³rico recargas | CPF + TransaÃ§Ã£o | ~100M | CSV â†’ S3 Bronze |
| `book_atraso` | HistÃ³rico atrasos | CPF + Evento | ~5M | Parquet â†’ S3 Bronze |
| `book_pagamento` | HistÃ³rico pagamentos | CPF + TransaÃ§Ã£o | ~80M | Parquet â†’ S3 Bronze |

**Processo de IngestÃ£o**:
- **AWS Lambda** executa ingestÃ£o de cada fonte
- ConversÃ£o para **Parquet** (formato otimizado)
- Particionamento por **data de ingestÃ£o**
- Storage em **S3 Bronze**

### 2. Books de VariÃ¡veis (Metodologia Claro)

**Conceito de Books**:
Estruturas prÃ©-calculadas de variÃ¡veis categorizadas por assunto, desenvolvidas para:

âœ… **PadronizaÃ§Ã£o**: EstruturaÃ§Ã£o consistente por tema  
âœ… **EficiÃªncia**: Maximizar cÃ¡lculo e reutilizaÃ§Ã£o de variÃ¡veis  
âœ… **Escalabilidade**: Disseminar uso para outras Ã¡reas da empresa  
âœ… **GovernanÃ§a**: Garantir qualidade e rastreabilidade

**Books Utilizados**:

| Book | ConteÃºdo | Uso no Modelo |
|------|----------|---------------|
| `book_atraso` | PadrÃµes de atraso | Comportamento de risco |
| `book_pagamento` | HistÃ³rico transacional | Capacidade de pagamento |
| `book_recarga` | PadrÃµes de recarga | **Diferencial**: Comportamento prÃ©-pago |

### 3. CatalogaÃ§Ã£o (Glue Crawler)

```bash
# Glue Crawler descobre automaticamente:
- Esquemas das tabelas
- Tipos de dados
- PartiÃ§Ãµes
- EstatÃ­sticas

# Output: AWS Glue Data Catalog
```

### 4. TransformaÃ§Ã£o Bronze â†’ Silver

**Objetivo**: Limpar e validar dados

**Tecnologias**:
- **AWS Lambda**: TransformaÃ§Ãµes leves (<15 min)
- **Amazon EMR (PySpark)**: TransformaÃ§Ãµes pesadas (>15 min, big data)

**Processos Aplicados**:

#### A. Data Quality Checks
```python
# ValidaÃ§Ãµes aplicadas
- CPF vÃ¡lido (algoritmo validador)
- CPF Ãºnico por safra
- Datas consistentes (formato, range)
- Valores dentro de limites esperados
- Campos obrigatÃ³rios preenchidos
```

#### B. Missing Values
| Tipo | EstratÃ©gia |
|------|-----------|
| CategÃ³ricas | Moda ou categoria "Desconhecido" |
| NumÃ©ricas contÃ­nuas | Mediana (robusta a outliers) |
| NumÃ©ricas discretas | Moda ou modelo preditivo |
| Flags booleanas | False (conservador) |

#### C. Outliers
```python
# DetecÃ§Ã£o: IQR (Interquartile Range)
Q1 = percentile(25)
Q3 = percentile(75)
IQR = Q3 - Q1
outlier_min = Q1 - 1.5 * IQR
outlier_max = Q3 + 1.5 * IQR

# Tratamento contextual:
- WinsorizaÃ§Ã£o (cap valores extremos)
- TransformaÃ§Ã£o log
- CategorizaÃ§Ã£o em bins
```

#### D. DeduplicaÃ§Ã£o
```python
# Regra de negÃ³cio Claro:
- Chave primÃ¡ria: CPF + Safra
- Ãšltimo registro em caso de duplicata (most recent)
```

#### E. Filtros de Grupo Controle
```python
# ExclusÃµes obrigatÃ³rias:
- Ã“bitos (fonte: bureau)
- Menores de 18 anos

# SeleÃ§Ã£o estatÃ­stica:
- 6Âº e 7Âº dÃ­gitos do CPF â†’ ~2% da base
- DistribuiÃ§Ã£o balanceada por UF
```

#### F. Cardinalidade
```python
# Evitar variÃ¡veis com cardinalidade extrema
# Threshold: > 293.000 valores Ãºnicos
# Exemplo: ID transaÃ§Ã£o especÃ­fico de recarga
# AÃ§Ã£o: Remover ou agregar
```

**Output**: Dados em **S3 Silver** (Parquet otimizado)

### 5. AgregaÃ§Ã£o Silver â†’ Gold

**Objetivo**: Criar visÃ£o Ãºnica por CPF + features

**Tecnologias**:
- **AWS Lambda**: AgregaÃ§Ãµes simples
- **Amazon EMR (PySpark)**: Feature engineering complexo

**Processos Aplicados**:

#### A. VisÃ£o Ãšnica por CPF

```sql
-- EstratÃ©gia de integraÃ§Ã£o
SELECT 
    cpf,
    safra,
    -- Dados cadastrais (1:1)
    cadastro.*,
    
    -- Scores (last value)
    LAST_VALUE(score) as score_bureau,
    
    -- AgregaÃ§Ãµes de recarga (Ãºltimos 3, 6, 12 meses)
    SUM(CASE WHEN data >= safra - 90 THEN valor END) as recarga_3m,
    SUM(CASE WHEN data >= safra - 180 THEN valor END) as recarga_6m,
    SUM(CASE WHEN data >= safra - 365 THEN valor END) as recarga_12m,
    
    -- AgregaÃ§Ãµes de pagamento
    COUNT(CASE WHEN atraso > 0 THEN 1 END) as qtd_atrasos,
    AVG(valor_pagamento) as ticket_medio_pag
    
FROM bronze_tables
LEFT JOIN ...
GROUP BY cpf, safra
```

#### B. Feature Engineering

**Categorias de Features Criadas**:

| Categoria | Exemplos | Justificativa |
|-----------|----------|---------------|
| **Cadastrais** | idade, tempo_cliente, regiao, classe_social | Perfil demogrÃ¡fico |
| **Comportamentais** | frequencia_recarga, consistencia_recarga, sazonalidade | **Diferencial telco** |
| **Financeiras** | score_bureau, qtd_atrasos, valor_medio_atraso | HistÃ³rico de crÃ©dito |
| **Telco** | uso_dados_mb, minutos_mes, servicos_adicionais | Engajamento com serviÃ§os |
| **Temporais** | recarga_3m, recarga_6m, recarga_12m, tendencia | PadrÃµes temporais |
| **RazÃµes** | recarga_renda, pagamento_recarga, atraso_pagamento | Capacidade relativa |
| **Flags** | flag_atraso_recorrente, flag_recarga_irregular | Indicadores de risco |

**Exemplo de Features AvanÃ§adas**:

```python
# TendÃªncia de recarga (regressÃ£o linear Ãºltimos 6 meses)
tendencia_recarga = slope(recargas_ultimos_6_meses)

# ConsistÃªncia de comportamento (coeficiente de variaÃ§Ã£o)
consistencia_recarga = std(recargas) / mean(recargas)

# Score composto
score_telco = (
    0.4 * norm(frequencia_recarga) +
    0.3 * norm(valor_medio_recarga) +
    0.3 * (1 - norm(cv_recarga))
)
```

**Output**: Dados em **S3 Gold** (Parquet otimizado, particionado por safra)

### 6. DefiniÃ§Ã£o de Target

```python
# Regra de negÃ³cio para inadimplÃªncia
# (Validada com patrocinadora)

target = (
    (dias_atraso > 30) |  # Atraso superior a 30 dias
    (valor_devido > threshold_negocio)  # Valor acima do limite
).astype(int)

# 0 = Adimplente (bom pagador)
# 1 = Inadimplente (mau pagador)
```

### 7. Safras de Modelagem

**Estrutura de Safras**:

| Safra | PerÃ­odo | Uso | Tamanho |
|-------|---------|-----|---------|
| Safra 1 | MÃªs 1 | Treino | ~1.5M CPFs |
| Safra 2 | MÃªs 2 | Treino | ~1.5M CPFs |
| Safra 3 | MÃªs 3 | Treino | ~1.5M CPFs |
| Safra 4 | MÃªs 4 | Treino | ~1.5M CPFs |
| **Safra OOT** | **Fev/Mar** | **ValidaÃ§Ã£o Final** | **~1.5M CPFs** |

**Split de Dados**:
- **Train**: 70% das 4 safras de treino
- **Validation**: 30% das 4 safras de treino
- **Test (Out-of-Time)**: Safra Fev/Mar completa

---

## ğŸ¤– Modelagem

### EstratÃ©gia de Modelagem Incremental

A Claro exige **modelagem incremental** para avaliar ROI de cada fonte de dados.

#### Processo Passo a Passo

```python
# 1. BASELINE - Apenas CPF + Safra + Target
features_v1 = ['cpf', 'safra']
modelo_v1 = treinar_modelo(features_v1, target)
ks_v1 = avaliar(modelo_v1)  # KS baseline

# 2. +BOOK ATRASO/PAGAMENTO
features_v2 = features_v1 + book_atraso + book_pagamento
modelo_v2 = treinar_modelo(features_v2, target)
ks_v2 = avaliar(modelo_v2)  # Exemplo: KS = 15
ganho_incremental_v2 = ks_v2 - ks_v1

# 3. +BOOK RECARGA
features_v3 = features_v2 + book_recarga
modelo_v3 = treinar_modelo(features_v3, target)
ks_v3 = avaliar(modelo_v3)  # Exemplo: KS = 25 (ganho de 10 pontos!)
ganho_incremental_v3 = ks_v3 - ks_v2

# 4. +DADOS CADASTRAIS
features_v4 = features_v3 + dados_cadastrais
modelo_v4 = treinar_modelo(features_v4, target)
ks_v4 = avaliar(modelo_v4)
ganho_incremental_v4 = ks_v4 - ks_v3

# 5. +DADOS TELCO
features_v5 = features_v4 + dados_telco
modelo_v5 = treinar_modelo(features_v5, target)
ks_v5 = avaliar(modelo_v5)
ganho_incremental_v5 = ks_v5 - ks_v4

# 6. +SCORES BUREAU EXTERNOS
features_v6 = features_v5 + scores_bureau
modelo_final = treinar_modelo(features_v6, target)
ks_final = avaliar(modelo_final)  # Objetivo: >= 33.1
ganho_incremental_v6 = ks_final - ks_v5
```

#### Justificativa de NegÃ³cio

> *"A empresa nÃ£o tem dinheiro infinito... a gente vai comprar dados de fornecedores, a gente vai adicionando os dados aos poucos, fonte a fonte, sempre buscamos os melhores. Garimpa as melhores fontes de dados e compra as que mais incrementam ao modelo e trazem resultados significativos."*

**AnÃ¡lise de Viabilidade**:
```python
# Para cada fonte de dados:
custo_aquisicao = valor_mensal_fornecedor
ganho_ks = ks_com_fonte - ks_sem_fonte
ganho_financeiro = (reducao_inadimplencia * base_clientes * ticket_medio)

roi = (ganho_financeiro - custo_aquisicao) / custo_aquisicao

# DecisÃ£o: adicionar fonte se ROI > threshold (ex: 200%)
```

### Algoritmos Candidatos

| Algoritmo | Vantagens | Desvantagens | Uso |
|-----------|-----------|--------------|-----|
| **RegressÃ£o LogÃ­stica** | InterpretÃ¡vel, rÃ¡pido | Assume linearidade | Baseline |
| **Random Forest** | Robusto, nÃ£o-linear | Menos interpretÃ¡vel | ComparaÃ§Ã£o |
| **XGBoost** | Alta performance, regularizaÃ§Ã£o | Tunning complexo | **Modelo final candidato** |
| **LightGBM** | RÃ¡pido, eficiente | SensÃ­vel a overfitting | Alternativa |

### HiperparÃ¢metros (Exemplo XGBoost)

```python
param_grid = {
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'n_estimators': [100, 200, 500],
    'min_child_weight': [1, 3, 5],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [0, 0.1, 1]
}

# GridSearch com CV=5 estratificado
best_model = GridSearchCV(
    XGBClassifier(objective='binary:logistic', random_state=42),
    param_grid,
    cv=StratifiedKFold(n_splits=5),
    scoring='roc_auc',
    n_jobs=-1
)
```

### Tratamento de Desbalanceamento

```python
# AnÃ¡lise de proporÃ§Ã£o de classes
print(target.value_counts(normalize=True))
# 0 (bom): 85%
# 1 (mau): 15%  â†’ Desbalanceado!

# EstratÃ©gias aplicadas:
# 1. Class weights
class_weight = {0: 1, 1: 5.67}  # 85/15

# 2. SMOTE (Synthetic Minority Over-sampling)
from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_train, y_train)

# 3. Threshold adjustment (otimizar F1 ou Recall)
threshold_optimal = find_optimal_threshold(y_val, y_pred_proba)
```

---

## ğŸ“Š ValidaÃ§Ã£o e MÃ©tricas

### EstratÃ©gia de ValidaÃ§Ã£o

#### 1. Cross-Validation Estratificado
```python
from sklearn.model_selection import StratifiedKFold

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    X_train_fold = X[train_idx]
    y_train_fold = y[train_idx]
    X_val_fold = X[val_idx]
    y_val_fold = y[val_idx]
    
    # Treinar e avaliar
    modelo.fit(X_train_fold, y_train_fold)
    ks_fold = calcular_ks(y_val_fold, modelo.predict_proba(X_val_fold))
    
print(f"KS mÃ©dio CV: {np.mean(ks_folds):.3f} Â± {np.std(ks_folds):.3f}")
```

#### 2. ValidaÃ§Ã£o Out-of-Time (CrÃ­tico!)

```python
# Treino: Safras 1-4 (temporal)
X_train = dados[dados['safra'].isin([1, 2, 3, 4])]
y_train = target[dados['safra'].isin([1, 2, 3, 4])]

# Teste: Safra Fev/Mar (OOT)
X_test_oot = dados[dados['safra'] == 'fev_mar']
y_test_oot = target[dados['safra'] == 'fev_mar']

# AvaliaÃ§Ã£o final
modelo_final.fit(X_train, y_train)
ks_oot = calcular_ks(y_test_oot, modelo_final.predict_proba(X_test_oot))

print(f"KS Out-of-Time: {ks_oot:.3f}")
print(f"Benchmark: 33.1")
print(f"Ganho: {ks_oot - 33.1:.3f} pontos")
```

### MÃ©tricas Principais

#### 1. KS Statistic (Kolmogorov-Smirnov)

```python
def calcular_ks(y_true, y_pred_proba):
    """
    Calcula KS: mÃ¡xima separaÃ§Ã£o entre distribuiÃ§Ãµes cumulativas
    de bons e maus pagadores
    """
    # Ordenar por probabilidade predita
    df = pd.DataFrame({
        'target': y_true,
        'prob': y_pred_proba[:, 1]  # Prob classe 1 (mau)
    }).sort_values('prob', ascending=False)
    
    # DistribuiÃ§Ãµes cumulativas
    df['bons_acum'] = (df['target'] == 0).cumsum() / (df['target'] == 0).sum()
    df['maus_acum'] = (df['target'] == 1).cumsum() / (df['target'] == 1).sum()
    
    # KS = mÃ¡xima separaÃ§Ã£o
    ks = (df['maus_acum'] - df['bons_acum']).abs().max()
    
    return ks * 100  # Retorna em %
```

**InterpretaÃ§Ã£o**:
- KS = 0: Modelo nÃ£o discrimina (aleatÃ³rio)
- KS = 20-30: DiscriminaÃ§Ã£o fraca
- KS = 30-50: **DiscriminaÃ§Ã£o boa** âœ…
- KS > 50: DiscriminaÃ§Ã£o excelente
- **Benchmark Claro: 33,1**

#### 2. GINI Coefficient

```python
from sklearn.metrics import roc_auc_score

def calcular_gini(y_true, y_pred_proba):
    """
    GINI = 2 * AUC - 1
    """
    auc = roc_auc_score(y_true, y_pred_proba[:, 1])
    gini = 2 * auc - 1
    return gini
```

#### 3. Curva ROC e AnÃ¡lise de Faixas

```python
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt

fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba[:, 1])

plt.figure(figsize=(10, 6))
plt.plot(fpr, tpr, label=f'Modelo (AUC={auc:.3f})')
plt.plot([0, 1], [0, 1], 'k--', label='Baseline (AUC=0.5)')

# Destacar regiÃ£o de interesse (metade inferior da curva)
plt.axvspan(0, 0.5, alpha=0.2, color='red', 
            label='RegiÃ£o CrÃ­tica (maior impacto)')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate (Recall)')
plt.title('Curva ROC - Foco na Metade Inferior')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()
```

> **Insight Claro**: *"O impacto sempre vai ser maior da metade da curva para baixo"* - Foco na discriminaÃ§Ã£o de clientes de mÃ©dio/alto risco, nÃ£o nos que jÃ¡ seriam aprovados de qualquer forma.

#### 4. Matriz de ConfusÃ£o e MÃ©tricas Derivadas

```python
from sklearn.metrics import confusion_matrix, classification_report

# Encontrar threshold Ã³timo (maximiza F1 ou balanceia Precision/Recall)
threshold_optimal = 0.3  # Exemplo

y_pred = (y_pred_proba[:, 1] >= threshold_optimal).astype(int)

cm = confusion_matrix(y_true, y_pred)
print("Matriz de ConfusÃ£o:")
print(cm)

print("\nClassification Report:")
print(classification_report(y_true, y_pred, 
                            target_names=['Bom Pagador', 'Mau Pagador']))
```

#### 5. Swap In / Swap Out Analysis

```python
def swap_analysis(y_true, y_pred_proba_atual, y_pred_proba_novo, 
                  threshold=0.5):
    """
    Analisa quantos clientes mudam de decisÃ£o com novo modelo
    """
    decisao_atual = (y_pred_proba_atual >= threshold).astype(int)
    decisao_novo = (y_pred_proba_novo >= threshold).astype(int)
    
    swap_in = ((decisao_atual == 0) & (decisao_novo == 1)).sum()
    swap_out = ((decisao_atual == 1) & (decisao_novo == 0)).sum()
    
    # Performance de cada grupo
    swap_in_bad_rate = y_true[(decisao_atual == 0) & (decisao_novo == 1)].mean()
    swap_out_bad_rate = y_true[(decisao_atual == 1) & (decisao_novo == 0)].mean()
    
    return {
        'swap_in': swap_in,
        'swap_out': swap_out,
        'swap_in_bad_rate': swap_in_bad_rate,
        'swap_out_bad_rate': swap_out_bad_rate
    }
```

#### 6. Taxa de AprovaÃ§Ã£o vs InadimplÃªncia (Trade-off)

```python
def tradeoff_analysis(y_true, y_pred_proba, thresholds=np.arange(0, 1, 0.05)):
    """
    Analisa trade-off entre aprovaÃ§Ã£o e inadimplÃªncia
    """
    results = []
    
    for threshold in thresholds:
        y_pred = (y_pred_proba[:, 1] >= threshold).astype(int)
        
        aprovados = (y_pred == 0).sum()
        taxa_aprovacao = aprovados / len(y_pred)
        
        inadimplentes = y_true[y_pred == 0].sum()
        taxa_inadimplencia = inadimplentes / aprovados if aprovados > 0 else 0
        
        results.append({
            'threshold': threshold,
            'taxa_aprovacao': taxa_aprovacao,
            'taxa_inadimplencia': taxa_inadimplencia
        })
    
    return pd.DataFrame(results)

# Visualizar
df_tradeoff = tradeoff_analysis(y_true, y_pred_proba)

fig, ax1 = plt.subplots(figsize=(12, 6))

ax1.plot(df_tradeoff['threshold'], df_tradeoff['taxa_aprovacao'], 
         'b-', label='Taxa de AprovaÃ§Ã£o')
ax1.set_xlabel('Threshold')
ax1.set_ylabel('Taxa de AprovaÃ§Ã£o', color='b')

ax2 = ax1.twinx()
ax2.plot(df_tradeoff['threshold'], df_tradeoff['taxa_inadimplencia'], 
         'r-', label='Taxa de InadimplÃªncia')
ax2.set_ylabel('Taxa de InadimplÃªncia', color='r')

# Marcar ponto de baseline (73-74%)
ax1.axhline(y=0.73, color='b', linestyle='--', alpha=0.5, 
            label='Baseline AprovaÃ§Ã£o (73%)')

plt.title('Trade-off: AprovaÃ§Ã£o vs InadimplÃªncia')
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()
```

### Feature Importance

```python
# XGBoost feature importance
import xgboost as xgb

feature_importance = pd.DataFrame({
    'feature': feature_names,
    'importance': modelo.feature_importances_
}).sort_values('importance', ascending=False)

# Top 20 features
print(feature_importance.head(20))

# VisualizaÃ§Ã£o
plt.figure(figsize=(10, 12))
plt.barh(feature_importance.head(20)['feature'], 
         feature_importance.head(20)['importance'])
plt.xlabel('Importance')
plt.title('Top 20 Features - XGBoost')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.show()

# SHAP values para interpretabilidade avanÃ§ada
import shap

explainer = shap.TreeExplainer(modelo)
shap_values = explainer.shap_values(X_test)

shap.summary_plot(shap_values, X_test, feature_names=feature_names)
```

---

## ğŸ”’ SeguranÃ§a e GovernanÃ§a

### Perfis de Acesso (IAM)

A arquitetura implementa 3 perfis distintos de acesso:

| Perfil | PermissÃµes | Casos de Uso |
|--------|------------|--------------|
| **Tomador de DecisÃ£o** | Leitura Gold, Athena, QuickSight | Executivos, gestores |
| **AnalÃ­tico** | Leitura Silver/Gold, Athena, SageMaker | Data Scientists, Analistas |
| **Operacional** | Leitura/Escrita Bronze/Silver/Gold, Lambda, EMR, Glue | Engenheiros de Dados |

### Criptografia

**At Rest (KMS)**:
```python
# S3 Bucket encryption
s3_client.put_bucket_encryption(
    Bucket='claro-hackathon-gold',
    ServerSideEncryptionConfiguration={
        'Rules': [{
            'ApplyServerSideEncryptionByDefault': {
                'SSEAlgorithm': 'aws:kms',
                'KMSMasterKeyID': KMS_KEY_ID
            }
        }]
    }
)
```

**In Transit (TLS)**:
- Todas as comunicaÃ§Ãµes AWS usam HTTPS/TLS 1.2+
- Certificados gerenciados automaticamente

### Auditoria e Monitoramento

**CloudWatch Logs**:
- Logs de execuÃ§Ã£o Lambda
- Logs EMR (steps, jobs)
- Logs Glue (crawlers, jobs)
- Logs SageMaker (training, inference)

**CloudWatch Alarms**:
```python
# Exemplo: Alarme de falha em pipeline
cloudwatch.put_metric_alarm(
    AlarmName='claro-pipeline-failure',
    MetricName='LambdaErrors',
    Namespace='AWS/Lambda',
    Statistic='Sum',
    Period=300,
    EvaluationPeriods=1,
    Threshold=1,
    ComparisonOperator='GreaterThanThreshold',
    AlarmActions=[SNS_TOPIC_ARN]
)
```

### Data Quality Checks

```python
# ValidaÃ§Ãµes automÃ¡ticas em cada camada
def data_quality_checks(df, layer):
    """
    ValidaÃ§Ãµes de qualidade de dados
    """
    checks = {
        'row_count': len(df) > 0,
        'null_cpf': df['cpf'].isnull().sum() == 0,
        'duplicate_cpf_safra': df.duplicated(['cpf', 'safra']).sum() == 0,
        'valid_cpf': df['cpf'].apply(validar_cpf).all(),
        'valid_dates': (df['safra'] >= '2020-01-01').all(),
        'target_binary': df['target'].isin([0, 1]).all() if 'target' in df else True
    }
    
    # Logging
    for check, passed in checks.items():
        status = 'PASSED' if passed else 'FAILED'
        logger.info(f"{layer} - {check}: {status}")
        
        if not passed:
            raise DataQualityException(f"{check} failed in {layer}")
    
    return all(checks.values())
```

---

## ğŸ“ˆ Resultados

> âš ï¸ **Status**: Modelo em desenvolvimento - Resultados serÃ£o atualizados apÃ³s treinamento completo

### Objetivos de Performance

| MÃ©trica | Baseline Atual | Benchmark | Objetivo | Status |
|---------|---------------|-----------|----------|---------|
| **KS Statistic** | - | 33,1 | â‰¥ 33,1 | ğŸ”„ Em desenvolvimento |
| **GINI** | - | - | MÃ¡ximo | ğŸ”„ Em desenvolvimento |
| **Taxa de AprovaÃ§Ã£o** | - | 73-74% | â‰¥ 73% | ğŸ”„ Em desenvolvimento |
| **Taxa de InadimplÃªncia** | - | Baseline GC | â‰¤ Baseline | ğŸ”„ Em desenvolvimento |
| **AUC-ROC** | - | - | â‰¥ 0.75 | ğŸ”„ Em desenvolvimento |

### Ganhos Incrementais por Fonte

*SerÃ¡ atualizado durante modelagem incremental*

| VersÃ£o | Fontes Adicionadas | KS | Ganho Î”KS | ROI Estimado |
|--------|-------------------|----|-----------|--------------| 
| v1 | Baseline (CPF + Safra) | - | - | - |
| v2 | + Book Atraso/Pagamento | - | - | - |
| v3 | + Book Recarga | - | - | - |
| v4 | + Dados Cadastrais | - | - | - |
| v5 | + Dados TELCO | - | - | - |
| v6 | + Scores Bureau | - | - | - |

### Feature Importance

*SerÃ¡ atualizado apÃ³s treinamento final*

**Top 10 Features Esperadas**:
1. TBD
2. TBD
3. ...

### AnÃ¡lise de NegÃ³cio

**Impacto Esperado**:
- ğŸ“ˆ Aumento na taxa de aprovaÃ§Ã£o mantendo risco
- ğŸ“‰ ReduÃ§Ã£o de inadimplÃªncia em X%
- ğŸ’° Ganho financeiro estimado: R$ X milhÃµes/ano
- ğŸ¯ IdentificaÃ§Ã£o de X mil clientes prÃ© â†’ pÃ³s elegÃ­veis

---

## ğŸ‘¥ Time

### Super Hackathon 2025 - Equipe Behavior Claro

**LÃ­der**: Rafael Lima

**Membros**:
- Daniel Dayan
- Diego Lessa
- Gabriel Lenhard
- Gabriel Roledo
- Tiago Carvalho
- CÃ©zar Augusto Freitas
- Grazy Miranda
- Ricardo Max

### PapÃ©is e Responsabilidades

- **Data Engineering**: Arquitetura AWS, pipeline de dados (Bronze â†’ Silver â†’ Gold)
- **Feature Engineering**: CriaÃ§Ã£o de books de variÃ¡veis e features preditivas
- **Modeling**: Desenvolvimento de modelos incrementais e otimizaÃ§Ã£o
- **Infrastructure**: Provisionamento e configuraÃ§Ã£o de recursos AWS
- **Visualization**: Dashboards Power BI e apresentaÃ§Ã£o de resultados
- **Documentation**: DocumentaÃ§Ã£o tÃ©cnica completa e reprodutÃ­vel

---

## ğŸ“š DocumentaÃ§Ã£o Completa

Para informaÃ§Ãµes detalhadas sobre cada etapa do projeto, consulte a pasta `docs/`:

- **[Contexto de NegÃ³cio](docs/01_business_context.md)**: Problema, objetivos, case de sucesso
- **[Entendimento dos Dados](docs/02_data_understanding.md)**: Bases, books de variÃ¡veis, estatÃ­sticas
- **[EDA e Insights](docs/03_eda_insights.md)**: AnÃ¡lise exploratÃ³ria detalhada
- **[Data Preparation](docs/04_data_preparation.md)**: Limpeza, validaÃ§Ã£o, grupo controle
- **[Feature Engineering](docs/05_feature_engineering.md)**: CriaÃ§Ã£o de variÃ¡veis preditivas
- **[Modelagem](docs/06_modeling.md)**: Processo incremental completo
- **[AvaliaÃ§Ã£o](docs/07_evaluation.md)**: MÃ©tricas, validaÃ§Ã£o out-of-time
- **[Arquitetura](docs/08_architecture.md)**: Detalhamento tÃ©cnico Medallion
- **[SeguranÃ§a](docs/09_security_governance.md)**: IAM, KMS, auditoria

### DicionÃ¡rio de Dados

O dicionÃ¡rio completo com todas as variÃ¡veis estÃ¡ disponÃ­vel em:
- `docs/data_dictionary.xlsx` - VersÃ£o Excel com metadados completos
- `docs/data_dictionary.md` - VersÃ£o markdown para consulta rÃ¡pida

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ“ Contato

Para dÃºvidas ou sugestÃµes sobre o projeto:

- **LÃ­der do Projeto**: Rafael Lima
- **Hackathon**: Super Hackathon 2025
- **Patrocinadora**: Claro

---

## ğŸ™ Agradecimentos

- **Claro** pela disponibilizaÃ§Ã£o dos dados reais e patrocÃ­nio do desafio
- **FÃ¡bio Oliveira** (Claro - CrÃ©dito e CobranÃ§a) pelas orientaÃ§Ãµes tÃ©cnicas
- **PoD Academy** pela organizaÃ§Ã£o do Super Hackathon 2025
- Todos os membros do time pela dedicaÃ§Ã£o e colaboraÃ§Ã£o

---

## ğŸ“Œ Roadmap

- [x] DefiniÃ§Ã£o do problema e objetivos âœ…
- [x] EstruturaÃ§Ã£o do projeto âœ…
- [x] Setup de infraestrutura AWS (Medallion) âœ…
- [x] Pipeline Bronze â†’ Silver â†’ Gold âœ…
- [ ] EDA completa de todas as bases ğŸ”„
- [ ] IntegraÃ§Ã£o e visÃ£o Ãºnica por CPF ğŸ”„
- [ ] Feature engineering ğŸ”„
- [ ] Modelagem incremental (6 versÃµes) ğŸ”„
- [ ] ValidaÃ§Ã£o out-of-time ğŸ”„
- [ ] OtimizaÃ§Ã£o de hiperparÃ¢metros ğŸ”„
- [ ] AnÃ¡lise de swap in/out ğŸ”„
- [ ] DocumentaÃ§Ã£o completa ğŸ”„
- [ ] Dashboards Power BI ğŸ”„
- [ ] ApresentaÃ§Ã£o final ğŸ”„

---

<div align="center">

**Desenvolvido com â¤ï¸ pela Equipe Behavior Claro**

Super Hackathon 2025

*"Transformando dados em oportunidades de negÃ³cio atravÃ©s de gestÃ£o inteligente de risco"*

</div>
